# IA_packages-synthesis

Conventions :  
B = batch size Â· T = longueur de sÃ©quence Â· F = features Â·  
U = unitÃ©s Â· H = hidden size Â· C = canaux Â·  
L = longueur 1D Â· H/W = hauteur / largeur Â·  
D = embedding dim Â· K = classes Â· O = output dim

## 1) Types de couches â€“ shapes, rÃ´le, implÃ©mentations
### ğŸ”¹ MLP (Dense / Fully Connected)

* RÃ´le

Transformation non linÃ©aire de features

Peut Ãªtre utilisÃ© instantanÃ©ment ou par pas de temps

* Input shape

Standard : (B, F)

Temporel (sans mÃ©lange) : (B, T, F)

* Output shape

(B, U)

* Temporel : (B, T, U)

âš ï¸ Point clÃ© (important)

Un MLP pytorch appliquÃ© sur (B, T, F) ne mÃ©lange pas le temps

Il agit indÃ©pendamment sur chaque xâ‚œ

Avec keras c'est Ã©quivalent Ã  TimeDistributed(MLP)
* ImplÃ©mentation
Keras
```python
Dense(
    units,
    activation=None,
    use_bias=True,
    kernel_initializer="glorot_uniform",
    bias_initializer="zeros",
    kernel_regularizer=None,
    bias_regularizer=None,
)
```
```python
PyTorch
nn.Linear(
    in_features,
    out_features,
    bias=True
)
```
### ğŸ”¹ CNN (Convolutional Neural Network)
#### CNN 1D (signaux, sÃ©ries)

* Input

Keras : (B, L, C)

PyTorch : (B, C, L)

* Output
  
Keras : (B,Loutâ€‹,Coutâ€‹)

PyTorch : (B,Coutâ€‹,Loutâ€‹)

* ImplÃ©mentation
Keras
```python
Conv1D(
    filters,
    kernel_size,
    strides=1,
    padding="valid",
    dilation_rate=1,
    activation=None,
    use_bias=True,
)
```
filters = nombre de filtre et donc = Cout le nombre de canaux en sortie

PyTorch
```python
nn.Conv1d(
    in_channels,
    out_channels,
    kernel_size,
    stride=1,
    padding=0,
    dilation=1,
    bias=True
)
```
#### CNN 2D (images)

* Input

Keras : (B, H, W, C)

PyTorch : (B, C, H, W)

* output

keras : (B,Houtâ€‹,Woutâ€‹,Coutâ€‹)

PyTorch : (B,Coutâ€‹,Houtâ€‹,Woutâ€‹)

* ImplÃ©mentation
Keras
```python
Conv2D(
    filters,
    kernel_size,
    strides=(1,1),
    padding="valid",
    activation=None
)
```
PyTorch
```python
nn.Conv2d(
    in_channels,
    out_channels,
    kernel_size,
    stride=1,
    padding=0
)
```
### ğŸ”¹ RNN (vanilla)

* RÃ´le

ModÃ©lisation sÃ©quentielle simple

DÃ©pendances temporelles courtes

* Input

(B, T, F)

* Output

(B, H) ou (B, T, H)
* ImplÃ©mentation
Keras
```python
SimpleRNN(
    units,
    activation="tanh",
    return_sequences=False,
    return_state=False,
    dropout=0.0,
    recurrent_dropout=0.0
)
```
PyTorch
```python
nn.RNN(
    input_size,
    hidden_size,
    num_layers=1,
    nonlinearity="tanh",
    batch_first=True,
    dropout=0.0,
    bidirectional=False
)
```
### ğŸ”¹ LSTM

* RÃ´le

DÃ©pendances longues

MÃ©moire explicite via câ‚œ

* Input

(B, T, F)

* Output

(B, H) ou (B, T, H)

Ã‰tats internes (hâ‚œ, câ‚œ)
* ImplÃ©mentation
```python
Keras
LSTM(
    units,
    activation="tanh",
    recurrent_activation="sigmoid",
    return_sequences=False,
    return_state=False,
    dropout=0.0,
    recurrent_dropout=0.0
)
```
PyTorch
```python
nn.LSTM(
    input_size,
    hidden_size,
    num_layers=1,
    batch_first=True,
    dropout=0.0,
    bidirectional=False
)
```
### ğŸ”¹ Transformer (Encoder)

* RÃ´le

DÃ©pendances longues sans rÃ©currence

Attention globale

Un bloc Transformer Encoder contient exactement :

Multi-Head Self-Attention

Add & Norm

Feed-Forward Network (FFN)

Add & Norm

* Input

(B, T, D)

* Output

(B, T, D)

-Keras
```python
MultiHeadAttention(
    num_heads,
    key_dim,
    value_dim=None,
    dropout=0.0
)
```
LayerNormalization
```python
self.norm1 = layers.LayerNormalization(epsilon=eps)
```
Dense (FFN)
```python
self.ffn1 = layers.Dense(d_ff, activation=activation)
self.ffn2 = layers.Dense(d_model)
(on essaie toujours de mettre au moins deux couches de FFN)
```
Le forward ressemblera typiquement Ã  :
```python
attn = self.mha(query=x, value=x, key=x, attention_mask=mask, training=training)
x = self.norm1(x + attn)   # Add & Norm
# Feed-forward
ffn = self.ffn2(self.ffn1(x))
x = self.norm2(x + ffn)
```
attention : keras ne fournis pas instinctivement le positional encoding il faut le rajouter nous mÃªme avant de ffaire rentrer l'embedding dans le modÃ¨le.


-PyTorch
d_model = taille d'embedding
```python
nn.TransformerEncoderLayer(
    d_model,
    nhead,
    dim_feedforward=2048,
    dropout=0.1,
    activation="relu",
    batch_first=True
)
```
on rajoute une couche de positionnal encoding, du dropout et une linear Ã  la fin :
```python
encoder = nn.TransformerEncoder(
    encoder_layer,
    num_layers=6
)
```

2) Couches de sortie selon la tÃ¢che
ğŸ”¹ Classification multi-classes (1 classe parmi K)

Sortie : Linear/Dense(K)

Activation : softmax (souvent dans la loss)

Limite : pas multi-label

ğŸ”¹ Classification binaire

Sortie : Linear(1)

Activation : sigmoid

Limite : sensible au dÃ©sÃ©quilibre

ğŸ”¹ Classification multi-label

Sortie : Linear(K)

Activation : sigmoid par classe

Limite : labels supposÃ©s indÃ©pendants

ğŸ”¹ RÃ©gression non bornÃ©e

Sortie : Linear(O)

Activation : aucune

Limite : valeurs physiquement impossibles possibles

ğŸ”¹ RÃ©gression bornÃ©e [0,1]

Sortie : Linear(O) + Sigmoid

Limite : saturation proche des bornes

ğŸ”¹ RÃ©gression positive

Sortie : Softplus ou ReLU

Limite : ReLU peut bloquer Ã  0

3) Losses utilisÃ©es dans la littÃ©rature
ğŸ”¹ RÃ©gression

MSE : standard, sensible aux outliers

MAE : robuste, convergence plus lente

Huber / SmoothL1 : compromis idÃ©al

NLL Gaussienne : prÃ©diction Î¼, Ïƒ

ğŸ”¹ Classification

CrossEntropy : multi-classes

Binary Cross Entropy : binaire / multi-label

Focal Loss : classes dÃ©sÃ©quilibrÃ©es

KL Divergence : distributions / distillation

ğŸ”¹ SÃ©quentiel spÃ©cifique

CTC Loss : sÃ©quences non alignÃ©es

Ranking / Contrastive : embeddings

4) Optimizers â€“ fonctionnement et usages
ğŸ”¹ SGD

Descente pure du gradient

Bonne gÃ©nÃ©ralisation

Lent, LR critique

ğŸ”¹ SGD + Momentum

Accumulation de vitesse

TrÃ¨s utilisÃ© en CNN vision

ğŸ”¹ Adam

Moments dâ€™ordre 1 et 2

Rapide, robuste

Standard pour RNN/LSTM

ğŸ”¹ AdamW

Adam + weight decay correct

Standard pour Transformers

TrÃ¨s bon gÃ©nÃ©raliste

ğŸ”¹ RMSProp

Moyenne mobile des gradientsÂ²

Historiquement utilisÃ© pour RNN

ğŸ”¹ Adagrad / Adadelta

Features rares

Peu utilisÃ©s aujourdâ€™hui

ğŸ”¹ LAMB / Adafactor

TrÃ¨s gros modÃ¨les

NLP / Transformers large-scale

5) Associations typiques observÃ©es

CNN (vision) â†’ SGD + momentum / Adam

RNN / LSTM â†’ Adam / RMSProp + gradient clipping

Transformer â†’ AdamW + scheduler + warmup

RÃ©gression â†’ Adam(W) + MSE / Huber

Classification â†’ Adam(W) + CE / BCE
