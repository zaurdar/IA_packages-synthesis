# IA_packages-synthesis

Conventions :  
B = batch size Â· T = longueur de sÃ©quence Â· F = features Â·  
U = unitÃ©s Â· H = hidden size Â· C = canaux Â·  
L = longueur 1D Â· H/W = hauteur / largeur Â·  
D = embedding dim Â· K = classes Â· O = output dim

## 1) Types de couches â€“ shapes, rÃ´le, implÃ©mentations
### ğŸ”¹ MLP (Dense / Fully Connected)

* RÃ´le

Transformation non linÃ©aire de features

Peut Ãªtre utilisÃ© instantanÃ©ment ou par pas de temps

* Input shape

Standard : (B, F)

Temporel (sans mÃ©lange) : (B, T, F)

* Output shape

(B, U)

* Temporel : (B, T, U)

âš ï¸ Point clÃ© (important)

Un MLP pytorch appliquÃ© sur (B, T, F) ne mÃ©lange pas le temps

Il agit indÃ©pendamment sur chaque xâ‚œ

Avec keras c'est Ã©quivalent Ã  TimeDistributed(MLP)
* ImplÃ©mentation
Keras
```python
Dense(
    units,
    activation=None,
    use_bias=True,
    kernel_initializer="glorot_uniform",
    bias_initializer="zeros",
    kernel_regularizer=None,
    bias_regularizer=None,
)
```
```python
PyTorch
nn.Linear(
    in_features,
    out_features,
    bias=True
)
```
### ğŸ”¹ CNN (Convolutional Neural Network)
#### CNN 1D (signaux, sÃ©ries)

* Input

Keras : (B, L, C)

PyTorch : (B, C, L)

* Output
  
Keras : (B,Loutâ€‹,Coutâ€‹)

PyTorch : (B,Coutâ€‹,Loutâ€‹)

* ImplÃ©mentation
Keras
```python
Conv1D(
    filters,
    kernel_size,
    strides=1,
    padding="valid",
    dilation_rate=1,
    activation=None,
    use_bias=True,
)
```
filters = nombre de filtre et donc = Cout le nombre de canaux en sortie

PyTorch
```python
nn.Conv1d(
    in_channels,
    out_channels,
    kernel_size,
    stride=1,
    padding=0,
    dilation=1,
    bias=True
)
```
#### CNN 2D (images)

* Input

Keras : (B, H, W, C)

PyTorch : (B, C, H, W)

* output

keras : (B,Houtâ€‹,Woutâ€‹,Coutâ€‹)

PyTorch : (B,Coutâ€‹,Houtâ€‹,Woutâ€‹)

* ImplÃ©mentation
Keras
```python
Conv2D(
    filters,
    kernel_size,
    strides=(1,1),
    padding="valid",
    activation=None
)
```
PyTorch
```python
nn.Conv2d(
    in_channels,
    out_channels,
    kernel_size,
    stride=1,
    padding=0
)
```
### ğŸ”¹ RNN (vanilla)

* RÃ´le

ModÃ©lisation sÃ©quentielle simple

DÃ©pendances temporelles courtes

* Input

(B, T, F)

* Output

(B, H) ou (B, T, H)
* ImplÃ©mentation
Keras
```python
SimpleRNN(
    units,
    activation="tanh",
    return_sequences=False,
    return_state=False,
    dropout=0.0,
    recurrent_dropout=0.0
)
```
PyTorch
```python
nn.RNN(
    input_size,
    hidden_size,
    num_layers=1,
    nonlinearity="tanh",
    batch_first=True,
    dropout=0.0,
    bidirectional=False
)
```
### ğŸ”¹ LSTM

* RÃ´le

DÃ©pendances longues

MÃ©moire explicite via câ‚œ

* Input

(B, T, F)

* Output

(B, H) ou (B, T, H)
en fonction du role qu'on va lui donner :
(B,H,T) si on fait du many-to-many, ex: taging temporel
(B,H) si on fait du many-to-one, ex: la classification
Ã‰tats internes (hâ‚œ, câ‚œ)
* ImplÃ©mentation

Keras
-le paramÃ¨tre return_sequences permet de spÃ©cifier si on veux toute la sequence B,T,H ou juste B,H
```python
LSTM(
    units,
    activation="tanh",
    recurrent_activation="sigmoid",
    return_sequences=False,
    return_state=False,
    dropout=0.0,
    recurrent_dropout=0.0
)
```
PyTorch
-en pytorch il retourne automatiquement toute la sequence l'output a donc d'office la forme B,T,H
```python
nn.LSTM(
    input_size,
    hidden_size,
    num_layers=1,
    batch_first=True,
    dropout=0.0,
    bidirectional=False
)
```
### ğŸ”¹ BiLSTM
* input

En gÃ©nÃ©ral (Keras, et PyTorch avec batch_first=True) :

Input : (B, T, F)

* output

Un BiLSTM concatÃ¨ne forward+backward, donc la dimension cachÃ©e devient 2H.

Cas A â€” sortie Ã  chaque timestep :

Output seq : (B, T, 2H)

Cas B â€” sortie globale (dernier Ã©tat) :

Output last : (B, 2H) (souvent on prend le dernier vecteur de la sÃ©quence ou on pool)
* ImplÃ©mentation
Keras

ğŸ‘‰ En Keras, un BiLSTM nâ€™est pas une couche sÃ©parÃ©e, mais un wrapper Bidirectional autour dâ€™un LSTM.

return_sequences garde le mÃªme rÃ´le que pour LSTM

la dimension cachÃ©e est doublÃ©e automatiquement : 2H
```python
Bidirectional(
    LSTM(
        units,
        activation="tanh",
        recurrent_activation="sigmoid",
        return_sequences=False,
        return_state=False,
        dropout=0.0,
        recurrent_dropout=0.0
    ),
    merge_mode="concat"  # par dÃ©faut
)
```

PyTorch

ğŸ‘‰ En PyTorch, le BiLSTM est activÃ© via le paramÃ¨tre bidirectional=True.

PyTorch retourne toujours toute la sÃ©quence

la dimension cachÃ©e est aussi doublÃ©e automatiquement
```python
nn.LSTM(
    input_size,
    hidden_size,
    num_layers=1,
    batch_first=True,
    dropout=0.0,
    bidirectional=True
)
```
### ğŸ”¹ Transformer (Encoder)

* RÃ´le

DÃ©pendances longues sans rÃ©currence

Attention globale

Un bloc Transformer Encoder contient exactement :

Multi-Head Self-Attention

Add & Norm

Feed-Forward Network (FFN)

Add & Norm

* Input

(B, T, D)

* Output

(B, T, D)
* ImplÃ©mentation
  
-Keras
```python
MultiHeadAttention(
    num_heads,
    key_dim,
    value_dim=None,
    dropout=0.0
)
```
LayerNormalization
```python
self.norm1 = layers.LayerNormalization(epsilon=eps)
```
Dense (FFN)
```python
self.ffn1 = layers.Dense(d_ff, activation=activation)
self.ffn2 = layers.Dense(d_model)
(on essaie toujours de mettre au moins deux couches de FFN)
```
Le forward ressemblera typiquement Ã  :
```python
attn = self.mha(query=x, value=x, key=x, attention_mask=mask, training=training)
x = self.norm1(x + attn)   # Add & Norm
# Feed-forward
ffn = self.ffn2(self.ffn1(x))
x = self.norm2(x + ffn)
```
âš ï¸ : keras ne fournis pas instinctivement le positional encoding il faut le rajouter nous mÃªme avant de ffaire rentrer l'embedding dans le modÃ¨le.


-PyTorch
d_model = taille d'embedding
```python
nn.TransformerEncoderLayer(
    d_model,
    nhead,
    dim_feedforward=2048,
    dropout=0.1,
    activation="relu",
    batch_first=True
)
```
on rajoute une couche de positionnal encoding, du dropout et une linear Ã  la fin :
```python
encoder = nn.TransformerEncoder(
    encoder_layer,
    num_layers=6
)
```

## 2) Couches de sortie selon la tÃ¢che
### ğŸ”¹ Classification multi-classes (1 classe parmi K)

Sortie : Linear/Dense(K)

Activation : softmax (souvent dans la loss)

Limite : pas multi-label

### ğŸ”¹ Classification binaire

Sortie : Linear(1)

Activation : sigmoid

Limite : sensible au dÃ©sÃ©quilibre

### ğŸ”¹ Classification multi-label

Sortie : Linear(K)

Activation : sigmoid par classe

Limite : labels supposÃ©s indÃ©pendants

### ğŸ”¹ RÃ©gression non bornÃ©e

Sortie : Linear(O)

Activation : aucune

Limite : valeurs physiquement impossibles possibles

### ğŸ”¹ RÃ©gression bornÃ©e [0,1]

Sortie : Linear(O) + Sigmoid

Limite : saturation proche des bornes

### ğŸ”¹ RÃ©gression positive

Sortie : Softplus ou ReLU

Limite : ReLU peut bloquer Ã  0

## 3) Losses utilisÃ©es dans la littÃ©rature
### ğŸ”¹ RÃ©gression

MSE : standard, sensible aux outliers

MAE : robuste, convergence plus lente

Huber / SmoothL1 : compromis idÃ©al

NLL Gaussienne : prÃ©diction Î¼, Ïƒ

### ğŸ”¹ Classification

CrossEntropy : multi-classes

Binary Cross Entropy : binaire / multi-label

Focal Loss : classes dÃ©sÃ©quilibrÃ©es

KL Divergence : distributions / distillation

### ğŸ”¹ SÃ©quentiel spÃ©cifique

CTC Loss : sÃ©quences non alignÃ©es

Ranking / Contrastive : embeddings

## 4) Optimizers â€“ fonctionnement et usages
### ğŸ”¹ SGD

Descente pure du gradient

Bonne gÃ©nÃ©ralisation

Lent, LR critique

### ğŸ”¹ SGD + Momentum

Accumulation de vitesse

TrÃ¨s utilisÃ© en CNN vision

### ğŸ”¹ Adam

Moments dâ€™ordre 1 et 2

Rapide, robuste

Standard pour RNN/LSTM

### ğŸ”¹ AdamW

Adam + weight decay correct

Standard pour Transformers

TrÃ¨s bon gÃ©nÃ©raliste

### ğŸ”¹ RMSProp

Moyenne mobile des gradientsÂ²

Historiquement utilisÃ© pour RNN

### ğŸ”¹ Adagrad / Adadelta

Features rares

Peu utilisÃ©s aujourdâ€™hui

### ğŸ”¹ LAMB / Adafactor

TrÃ¨s gros modÃ¨les

NLP / Transformers large-scale
Dropout â€” fiche pratique

## 5) Dropout

Technique de rÃ©gularisation.

Pendant lâ€™entraÃ®nement, une fraction p des activations est mise Ã  zÃ©ro alÃ©atoirement.

Objectif : rÃ©duire lâ€™overfitting en empÃªchant la co-adaptation des neurones.

Inactif en inference (test).

### 1) ParamÃ¨tre clÃ©

dropout_rate = p avec p âˆˆ [0,1)

p = 0.1 â†’ 10 % des activations annulÃ©es

p = 0.5 â†’ 50 % annulÃ©es

Bonnes valeurs usuelles :

Transformers : 0.1

CNN : 0.2 â€“ 0.5

RNN / LSTM : 0.1 â€“ 0.3

MLP : 0.3 â€“ 0.5

### 2) ImplÃ©mentation en Keras
#### a) Dropout classique (MLP, CNN, Transformer)
from tensorflow.keras.layers import Dropout

x = Dense(128, activation="relu")(x)
x = Dropout(0.3)(x)


AppliquÃ© sur les activations

Actif uniquement quand training=True

#### b) Dropout dans un Transformer (aprÃ¨s sous-blocs)
attn_out = MultiHeadAttention(...)(x, x)
attn_out = Dropout(0.1)(attn_out)
x = LayerNormalization()(x + attn_out)

#### c) RNN / LSTM (spÃ©cifique)
LSTM(
    units=128,
    dropout=0.2,           # sur les entrÃ©es
    recurrent_dropout=0.0 # sur les connexions rÃ©currentes (souvent 0)
)

### 3) ImplÃ©mentation en PyTorch
#### a) Dropout classique
import torch.nn as nn

drop = nn.Dropout(p=0.3)

x = self.fc(x)
x = drop(x)

#### b) Dans un nn.Module
class Model(nn.Module):
    def __init__(self):
        super().__init__()
        self.fc = nn.Linear(128, 128)
        self.drop = nn.Dropout(0.3)

    def forward(self, x):
        x = self.fc(x)
        x = self.drop(x)
        return x

#### c) RNN / LSTM
nn.LSTM(
    input_size=F,
    hidden_size=H,
    num_layers=2,
    dropout=0.2,      # entre couches (pas sur les rÃ©currences internes)
    batch_first=True
)


âš ï¸ En PyTorch, le dropout du LSTM sâ€™applique entre les couches, pas Ã  lâ€™intÃ©rieur dâ€™une seule couche.

### 4) OÃ¹ placer le Dropout (rÃ¨gles simples)
âœ… Bonnes pratiques

AprÃ¨s une couche Dense / Linear

AprÃ¨s un sous-bloc Transformer (MHA, FFN), avant Add & Norm

Avant la tÃªte de sortie, jamais aprÃ¨s lâ€™activation finale

âŒ Ã€ Ã©viter

Juste avant une sortie softmax/sigmoid

Trop tÃ´t dans le rÃ©seau (perte dâ€™information)

Trop Ã©levÃ© dans les RNN (instabilitÃ© temporelle)

### 5) Contextes dâ€™utilisation
#### MLP

TrÃ¨s efficace

Souvent aprÃ¨s chaque couche Dense

#### CNN

Utile surtout aprÃ¨s les couches denses

Parfois remplacÃ© par SpatialDropout

#### RNN / LSTM

Ã€ utiliser avec parcimonie

PlutÃ´t sur les entrÃ©es / entre couches

#### Transformer

Standard dans :

MHA

FFN

chemins rÃ©siduels

Valeur canonique : 0.1

### 6) Ce que fait / ne fait pas le Dropout

Fait

RÃ©duit lâ€™overfitting

Force des reprÃ©sentations robustes

AmÃ©liore la gÃ©nÃ©ralisation

Ne fait pas

Ne supprime pas des neurones dÃ©finitivement

Ne modifie pas lâ€™architecture

Nâ€™agit pas en inference

## 5) Associations typiques observÃ©es

CNN (vision) â†’ SGD + momentum / Adam

RNN / LSTM â†’ Adam / RMSProp + gradient clipping

Transformer â†’ AdamW + scheduler + warmup

RÃ©gression â†’ Adam(W) + MSE / Huber

Classification â†’ Adam(W) + CE / BCE
