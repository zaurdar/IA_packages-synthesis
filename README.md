# IA_packages-synthesis

Conventions :  
B = batch size ¬∑ T = longueur de s√©quence ¬∑ F = features ¬∑  
U = unit√©s ¬∑ H = hidden size ¬∑ C = canaux ¬∑  
L = longueur 1D ¬∑ H/W = hauteur / largeur ¬∑  
D = embedding dim ¬∑ K = classes ¬∑ O = output dim

## 1) Types de couches ‚Äì shapes, r√¥le, impl√©mentations
### üîπ MLP (Dense / Fully Connected)

* R√¥le

Transformation non lin√©aire de features

Peut √™tre utilis√© instantan√©ment ou par pas de temps

* Input shape

Standard : (B, F)

Temporel (sans m√©lange) : (B, T, F)

* Output shape

(B, U)

* Temporel : (B, T, U)

‚ö†Ô∏è Point cl√© (important)

Un MLP pytorch appliqu√© sur (B, T, F) ne m√©lange pas le temps

Il agit ind√©pendamment sur chaque x‚Çú

Avec keras c'est √©quivalent √† TimeDistributed(MLP)
* Impl√©mentation
Keras
```python
Dense(
    units,
    activation=None,
    use_bias=True,
    kernel_initializer="glorot_uniform",
    bias_initializer="zeros",
    kernel_regularizer=None,
    bias_regularizer=None,
)
```
```python
PyTorch
nn.Linear(
    in_features,
    out_features,
    bias=True
)
```
### üîπ CNN (Convolutional Neural Network)
#### CNN 1D (signaux, s√©ries)

* Input

Keras : (B, L, C)

PyTorch : (B, C, L)

* Output
  
Keras : (B,Lout‚Äã,Cout‚Äã)

PyTorch : (B,Cout‚Äã,Lout‚Äã)

* Impl√©mentation

Keras
```python
Conv1D(
    filters,
    kernel_size,
    strides=1,
    padding="valid",
    dilation_rate=1,
    activation=None,
    use_bias=True,
)
```
filters = nombre de filtre et donc = Cout le nombre de canaux en sortie

PyTorch
```python
nn.Conv1d(
    in_channels,
    out_channels,
    kernel_size,
    stride=1,
    padding=0,
    dilation=1,
    bias=True
)
```
#### CNN 2D (images)

* Input

Keras : (B, H, W, C)

PyTorch : (B, C, H, W)

* output

keras : (B,Hout‚Äã,Wout‚Äã,Cout‚Äã)

PyTorch : (B,Cout‚Äã,Hout‚Äã,Wout‚Äã)


Pour faire du time distributed en keras c'est pareil que pour les MLP par contre en pytorch on recommende d'utiliser x.view(B*T,...) pour fuisonner la couche de batch et temporelle( on les remets apr√®s mais ainsi le cnn ne m√©lange pas les infos temporelles).

Dans le cas temporel on a donc :
Tenseurs d‚Äôentr√©e

PyTorch

(B, T, C, H, W)


Keras / TensorFlow

(B, T, H, W, C)
* Impl√©mentation
Keras
```python
Conv2D(
    filters,
    kernel_size,
    strides=(1,1),
    padding="valid",
    activation=None
)
```
PyTorch
```python
nn.Conv2d(
    in_channels,
    out_channels,
    kernel_size,
    stride=1,
    padding=0
)
```
### üîπ RNN (vanilla)

* R√¥le

Mod√©lisation s√©quentielle simple

D√©pendances temporelles courtes

* Input

(B, T, F)

* Output

(B, H) ou (B, T, H)
* Impl√©mentation
Keras
```python
SimpleRNN(
    units,
    activation="tanh",
    return_sequences=False,
    return_state=False,
    dropout=0.0,
    recurrent_dropout=0.0
)
```
PyTorch
```python
nn.RNN(
    input_size,
    hidden_size,
    num_layers=1,
    nonlinearity="tanh",
    batch_first=True,
    dropout=0.0,
    bidirectional=False
)
```
### üîπ LSTM

* R√¥le

D√©pendances longues

M√©moire explicite via c‚Çú

* Input

(B, T, F)

* Output

(B, H) ou (B, T, H)
en fonction du role qu'on va lui donner :
(B,H,T) si on fait du many-to-many, ex: taging temporel
(B,H) si on fait du many-to-one, ex: la classification
√âtats internes (h‚Çú, c‚Çú)
* Impl√©mentation

Keras
-le param√®tre return_sequences permet de sp√©cifier si on veux toute la sequence B,T,H ou juste B,H
```python
LSTM(
    units,
    activation="tanh",
    recurrent_activation="sigmoid",
    return_sequences=False,
    return_state=False,
    dropout=0.0,
    recurrent_dropout=0.0
)
```
PyTorch
-en pytorch il retourne automatiquement toute la sequence l'output a donc d'office la forme B,T,H
```python
nn.LSTM(
    input_size,
    hidden_size,
    num_layers=1,
    batch_first=True,
    dropout=0.0,
    bidirectional=False
)
```
### üîπ BiLSTM
* input

En g√©n√©ral (Keras, et PyTorch avec batch_first=True) :

Input : (B, T, F)

* output

Un BiLSTM concat√®ne forward+backward, donc la dimension cach√©e devient 2H.

Cas A ‚Äî sortie √† chaque timestep :

Output seq : (B, T, 2H)

Cas B ‚Äî sortie globale (dernier √©tat) :

Output last : (B, 2H) (souvent on prend le dernier vecteur de la s√©quence ou on pool)
* Impl√©mentation
Keras

üëâ En Keras, un BiLSTM n‚Äôest pas une couche s√©par√©e, mais un wrapper Bidirectional autour d‚Äôun LSTM.

return_sequences garde le m√™me r√¥le que pour LSTM

la dimension cach√©e est doubl√©e automatiquement : 2H
```python
Bidirectional(
    LSTM(
        units,
        activation="tanh",
        recurrent_activation="sigmoid",
        return_sequences=False,
        return_state=False,
        dropout=0.0,
        recurrent_dropout=0.0
    ),
    merge_mode="concat"  # par d√©faut
)
```

PyTorch

üëâ En PyTorch, le BiLSTM est activ√© via le param√®tre bidirectional=True.

PyTorch retourne toujours toute la s√©quence

la dimension cach√©e est aussi doubl√©e automatiquement
```python
nn.LSTM(
    input_size,
    hidden_size,
    num_layers=1,
    batch_first=True,
    dropout=0.0,
    bidirectional=True
)
```
### üîπ ConvLSTM
Un **ConvLSTM** (Convolutional LSTM) est une extension du LSTM classique con√ßue pour
traiter des **donn√©es spatio-temporelles** (s√©quences d‚Äôimages, cartes, champs 2D √©voluant dans le temps).

L‚Äôid√©e cl√© est simple :

> **on remplace toutes les op√©rations fully connected internes du LSTM par des convolutions**.

Ainsi :
- la **structure spatiale** (voisinage, motifs locaux) est conserv√©e,
- la **dynamique temporelle** est mod√©lis√©e via la m√©moire du LSTM.

Conceptuellement, un ConvLSTM combine :
- un **CNN** (pour l‚Äôespace),
- un **LSTM** (pour le temps),
mais de mani√®re **coupl√©e et locale**, et non s√©quentielle.

* input

√Ä chaque pas de temps, l‚Äôentr√©e est une carte spatiale (image / feature map).

En g√©n√©ral :

Keras / TensorFlow :

Input : (B, T, H, W, C)


PyTorch :

Input : (B, T, C, H, W)


o√π :

B : batch size

T : nombre de pas de temps

H, W : dimensions spatiales

C : canaux d‚Äôentr√©e

* output

Un ConvLSTM conserve la structure spatiale dans sa sortie.
La dimension cach√©e correspond au nombre de filtres convolutionnels F.

Cas A ‚Äî sortie √† chaque timestep :

Output seq : (B, T, H, W, F)


Cas B ‚Äî sortie finale uniquement :

Output last : (B, H, W, F)


üëâ Contrairement √† un LSTM/BiLSTM classique, la sortie n‚Äôest pas un vecteur, mais une carte 2D (feature map).

* Impl√©mentation

Keras

üëâ En Keras, le ConvLSTM est disponible nativement via ConvLSTM2D.

filters joue le r√¥le de la dimension cach√©e H

kernel_size d√©finit le voisinage spatial

return_sequences garde le m√™me r√¥le que pour LSTM
```python
ConvLSTM2D(
    filters,
    kernel_size=(3, 3),
    padding="same",
    activation="tanh",
    return_sequences=False,  # True -> (B, T, H, W, F)
    return_state=False,
    dropout=0.0,
    recurrent_dropout=0.0
)
```
PyTorch

üëâ PyTorch ne fournit pas de ConvLSTM natif.
Il faut l‚Äôimpl√©menter manuellement ou utiliser une librairie externe.
### üîπ Transformer (Encoder)

* R√¥le

D√©pendances longues sans r√©currence

Attention globale

Un bloc Transformer Encoder contient exactement :

Multi-Head Self-Attention

Add & Norm

Feed-Forward Network (FFN)

Add & Norm

* Input

(B, T, D)

* Output

(B, T, D)
* Impl√©mentation
  
-Keras
```python
MultiHeadAttention(
    num_heads,
    key_dim,
    value_dim=None,
    dropout=0.0
)
```
LayerNormalization
```python
self.norm1 = layers.LayerNormalization(epsilon=eps)
```
Dense (FFN)
```python
self.ffn1 = layers.Dense(d_ff, activation=activation)
self.ffn2 = layers.Dense(d_model)
(on essaie toujours de mettre au moins deux couches de FFN)
```
Le forward ressemblera typiquement √† :
```python
attn = self.mha(query=x, value=x, key=x, attention_mask=mask, training=training)
x = self.norm1(x + attn)   # Add & Norm
# Feed-forward
ffn = self.ffn2(self.ffn1(x))
x = self.norm2(x + ffn)
```
‚ö†Ô∏è : keras ne fournis pas instinctivement le positional encoding il faut le rajouter nous m√™me avant de ffaire rentrer l'embedding dans le mod√®le.


-PyTorch
d_model = taille d'embedding
```python
nn.TransformerEncoderLayer(
    d_model,
    nhead,
    dim_feedforward=2048,
    dropout=0.1,
    activation="relu",
    batch_first=True
)
```
on rajoute une couche de positionnal encoding, du dropout et une linear √† la fin :
```python
encoder = nn.TransformerEncoder(
    encoder_layer,
    num_layers=6
)
```

## 2) Couches de sortie selon la t√¢che
### üîπ Classification multi-classes (1 classe parmi K)

Sortie : Linear/Dense(K)

Activation : softmax (souvent dans la loss)

Limite : pas multi-label

### üîπ Classification binaire

Sortie : Linear(1)

Activation : sigmoid

Limite : sensible au d√©s√©quilibre

### üîπ Classification multi-label

Sortie : Linear(K)

Activation : sigmoid par classe

Limite : labels suppos√©s ind√©pendants

### üîπ R√©gression non born√©e

Sortie : Linear(O)

Activation : aucune

Limite : valeurs physiquement impossibles possibles

### üîπ R√©gression born√©e [0,1]

Sortie : Linear(O) + Sigmoid

Limite : saturation proche des bornes

### üîπ R√©gression positive

Sortie : Softplus ou ReLU

Limite : ReLU peut bloquer √† 0

## 3) Losses utilis√©es dans la litt√©rature
### üîπ R√©gression

MSE : standard, sensible aux outliers

MAE : robuste, convergence plus lente

Huber / SmoothL1 : compromis id√©al

NLL Gaussienne : pr√©diction Œº, œÉ

### üîπ Classification

CrossEntropy : multi-classes

Binary Cross Entropy : binaire / multi-label

Focal Loss : classes d√©s√©quilibr√©es

KL Divergence : distributions / distillation

### üîπ S√©quentiel sp√©cifique

CTC Loss : s√©quences non align√©es

Ranking / Contrastive : embeddings

## 4) Optimizers ‚Äì fonctionnement et usages
### üîπ SGD

Descente pure du gradient

Bonne g√©n√©ralisation

Lent, LR critique

### üîπ SGD + Momentum

Accumulation de vitesse

Tr√®s utilis√© en CNN vision

### üîπ Adam

Moments d‚Äôordre 1 et 2

Rapide, robuste

Standard pour RNN/LSTM

### üîπ AdamW

Adam + weight decay correct

Standard pour Transformers

Tr√®s bon g√©n√©raliste

### üîπ RMSProp

Moyenne mobile des gradients¬≤

Historiquement utilis√© pour RNN

### üîπ Adagrad / Adadelta

Features rares

Peu utilis√©s aujourd‚Äôhui

### üîπ LAMB / Adafactor

Tr√®s gros mod√®les

NLP / Transformers large-scale
Dropout ‚Äî fiche pratique

## 5) Dropout

Technique de r√©gularisation.

Pendant l‚Äôentra√Ænement, une fraction p des activations est mise √† z√©ro al√©atoirement.

Objectif : r√©duire l‚Äôoverfitting en emp√™chant la co-adaptation des neurones.

Inactif en inference (test).

### 1) Param√®tre cl√©

dropout_rate = p avec p ‚àà [0,1)

p = 0.1 ‚Üí 10 % des activations annul√©es

p = 0.5 ‚Üí 50 % annul√©es

Bonnes valeurs usuelles :

Transformers : 0.1

CNN : 0.2 ‚Äì 0.5

RNN / LSTM : 0.1 ‚Äì 0.3

MLP : 0.3 ‚Äì 0.5

### 2) Impl√©mentation en Keras
#### a) Dropout classique (MLP, CNN, Transformer)
from tensorflow.keras.layers import Dropout

x = Dense(128, activation="relu")(x)
x = Dropout(0.3)(x)


Appliqu√© sur les activations

Actif uniquement quand training=True

#### b) Dropout dans un Transformer (apr√®s sous-blocs)
attn_out = MultiHeadAttention(...)(x, x)
attn_out = Dropout(0.1)(attn_out)
x = LayerNormalization()(x + attn_out)

#### c) RNN / LSTM (sp√©cifique)
LSTM(
    units=128,
    dropout=0.2,           # sur les entr√©es
    recurrent_dropout=0.0 # sur les connexions r√©currentes (souvent 0)
)

### 3) Impl√©mentation en PyTorch
#### a) Dropout classique
import torch.nn as nn

drop = nn.Dropout(p=0.3)

x = self.fc(x)
x = drop(x)

#### b) Dans un nn.Module
class Model(nn.Module):
    def __init__(self):
        super().__init__()
        self.fc = nn.Linear(128, 128)
        self.drop = nn.Dropout(0.3)

    def forward(self, x):
        x = self.fc(x)
        x = self.drop(x)
        return x

#### c) RNN / LSTM
nn.LSTM(
    input_size=F,
    hidden_size=H,
    num_layers=2,
    dropout=0.2,      # entre couches (pas sur les r√©currences internes)
    batch_first=True
)


‚ö†Ô∏è En PyTorch, le dropout du LSTM s‚Äôapplique entre les couches, pas √† l‚Äôint√©rieur d‚Äôune seule couche.

### 4) O√π placer le Dropout (r√®gles simples)
‚úÖ Bonnes pratiques

Apr√®s une couche Dense / Linear

Apr√®s un sous-bloc Transformer (MHA, FFN), avant Add & Norm

Avant la t√™te de sortie, jamais apr√®s l‚Äôactivation finale

‚ùå √Ä √©viter

Juste avant une sortie softmax/sigmoid

Trop t√¥t dans le r√©seau (perte d‚Äôinformation)

Trop √©lev√© dans les RNN (instabilit√© temporelle)

### 5) Contextes d‚Äôutilisation
#### MLP

Tr√®s efficace

Souvent apr√®s chaque couche Dense

#### CNN

Utile surtout apr√®s les couches denses

Parfois remplac√© par SpatialDropout

#### RNN / LSTM

√Ä utiliser avec parcimonie

Plut√¥t sur les entr√©es / entre couches

#### Transformer

Standard dans :

MHA

FFN

chemins r√©siduels

Valeur canonique : 0.1

### 6) Ce que fait / ne fait pas le Dropout

Fait

R√©duit l‚Äôoverfitting

Force des repr√©sentations robustes

Am√©liore la g√©n√©ralisation

Ne fait pas

Ne supprime pas des neurones d√©finitivement

Ne modifie pas l‚Äôarchitecture

N‚Äôagit pas en inference

## 5) Associations typiques observ√©es

CNN (vision) ‚Üí SGD + momentum / Adam

RNN / LSTM ‚Üí Adam / RMSProp + gradient clipping

Transformer ‚Üí AdamW + scheduler + warmup

R√©gression ‚Üí Adam(W) + MSE / Huber

Classification ‚Üí Adam(W) + CE / BCE
## 6) √©stimer le nombre de couches n√©cessaires
### Estimation du nombre de couches n√©cessaires

* Principe fondamental
Il n‚Äôexiste pas de formule exacte pour d√©terminer le nombre de couches d‚Äôun r√©seau.
La profondeur doit √™tre choisie **en fonction de la structure du probl√®me**, et non
uniquement √† partir des dimensions d‚Äôentr√©e ou de sortie.

La profondeur permet de **factoriser la complexit√©** :
- couches basses : motifs simples,
- couches interm√©diaires : structures compos√©es,
- couches hautes : concepts abstraits.

---

### Heuristique 1 ‚Äî Complexit√© spatiale et structure des donn√©es

* Donn√©es simples
- signaux peu structur√©s,
- faible variabilit√©.

üëâ 1 √† 2 couches suffisent.

* Donn√©es structur√©es
- textures,
- motifs r√©p√©titifs,
- corr√©lations locales.

üëâ 3 √† 5 couches sont g√©n√©ralement n√©cessaires.

* Donn√©es tr√®s hi√©rarchiques
- structures complexes,
- d√©pendances multi-√©chelles.

üëâ 5 √† 10 couches ou plus, souvent avec connexions r√©siduelles.

---

### Heuristique 2 ‚Äî √âtendue des d√©pendances temporelles

* D√©pendances courtes
- variations locales,
- peu de m√©moire n√©cessaire.

üëâ Convolutions et pooling temporel suffisants.

* D√©pendances moyennes
- √©volution progressive,
- transitions temporelles claires.

üëâ Une couche LSTM ou BiLSTM.

* D√©pendances longues
- contexte global important,
- m√©moire sur de nombreux pas de temps.

üëâ Plusieurs couches r√©currentes, ou architectures √† attention.

---

### Heuristique 3 ‚Äî Taille du jeu de donn√©es

* Peu de donn√©es
- risque √©lev√© de sur-apprentissage.

üëâ R√©seau peu profond et fortement r√©gularis√©.

* Beaucoup de donn√©es
- grande diversit√©,
- meilleure g√©n√©ralisation possible.

üëâ R√©seau plus profond, avec normalisation et r√©gularisation adapt√©es.

---

### Heuristique 4 ‚Äî Nature de la sortie

* Sortie simple
- classification globale,
- r√©gression scalaire.

üëâ Peu de couches n√©cessaires.

* Sortie complexe
- pr√©diction par pas de temps,
- sorties structur√©es.

üëâ Plus de couches pour capter des relations fines.

---

### M√©thode pratique recommand√©e

1. Commencer par une **architecture simple**.
2. Observer les **courbes d‚Äôapprentissage**.
3. Ajouter des couches uniquement en cas de sous-apprentissage.
4. Arr√™ter l‚Äôaugmentation de profondeur d√®s que le gain devient marginal.

---

* Points importants

- Ajouter des couches augmente la capacit√©, mais aussi le risque d‚Äôoverfitting.
- La profondeur n‚Äôest utile que si elle correspond √† une structure r√©elle dans les donn√©es.
- La validation empirique reste indispensable.

---

* R√©sum√©

> Le nombre de couches d‚Äôun r√©seau doit √™tre choisi de mani√®re progressive et justifi√©e,
> en fonction de la complexit√© des motifs √† apprendre, des d√©pendances temporelles et
> de la quantit√© de donn√©es disponibles.
