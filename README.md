# IA_packages-synthesis
## 1) Types de couches â€“ shapes, rÃ´le, implÃ©mentations
### ğŸ”¹ MLP (Dense / Fully Connected)

* RÃ´le

Transformation non linÃ©aire de features

Peut Ãªtre utilisÃ© instantanÃ©ment ou par pas de temps

Input shape

Standard : (B, F)

Temporel (sans mÃ©lange) : (B, T, F)

Output shape

(B, U)

Temporel : (B, T, U)

âš ï¸ Point clÃ© (important)

Un MLP appliquÃ© sur (B, T, F) ne mÃ©lange pas le temps

Il agit indÃ©pendamment sur chaque xâ‚œ

Ã‰quivalent Ã  TimeDistributed(MLP)

Keras
```python
Dense(
    units,
    activation=None,
    use_bias=True,
    kernel_initializer="glorot_uniform",
    bias_initializer="zeros",
    kernel_regularizer=None,
    bias_regularizer=None,
)
```
```python
PyTorch
nn.Linear(
    in_features,
    out_features,
    bias=True
)
```
ğŸ”¹ CNN (Convolutional Neural Network)
CNN 1D (signaux, sÃ©ries)

Input

Keras : (B, L, C)

PyTorch : (B, C, L)

Output

(B, L', C_out)

Keras
Conv1D(
    filters,
    kernel_size,
    strides=1,
    padding="valid",
    dilation_rate=1,
    activation=None,
    use_bias=True,
)

PyTorch
nn.Conv1d(
    in_channels,
    out_channels,
    kernel_size,
    stride=1,
    padding=0,
    dilation=1,
    bias=True
)

CNN 2D (images)

Input

Keras : (B, H, W, C)

PyTorch : (B, C, H, W)

Keras
Conv2D(
    filters,
    kernel_size,
    strides=(1,1),
    padding="valid",
    activation=None
)

PyTorch
nn.Conv2d(
    in_channels,
    out_channels,
    kernel_size,
    stride=1,
    padding=0
)

ğŸ”¹ RNN (vanilla)

RÃ´le

ModÃ©lisation sÃ©quentielle simple

DÃ©pendances temporelles courtes

Input

(B, T, F)

Output

(B, H) ou (B, T, H)

Keras
SimpleRNN(
    units,
    activation="tanh",
    return_sequences=False,
    return_state=False,
    dropout=0.0,
    recurrent_dropout=0.0
)

PyTorch
nn.RNN(
    input_size,
    hidden_size,
    num_layers=1,
    nonlinearity="tanh",
    batch_first=True,
    dropout=0.0,
    bidirectional=False
)

ğŸ”¹ LSTM

RÃ´le

DÃ©pendances longues

MÃ©moire explicite via câ‚œ

Input

(B, T, F)

Output

(B, H) ou (B, T, H)

Ã‰tats internes (hâ‚œ, câ‚œ)

Keras
LSTM(
    units,
    activation="tanh",
    recurrent_activation="sigmoid",
    return_sequences=False,
    return_state=False,
    dropout=0.0,
    recurrent_dropout=0.0
)

PyTorch
nn.LSTM(
    input_size,
    hidden_size,
    num_layers=1,
    batch_first=True,
    dropout=0.0,
    bidirectional=False
)

ğŸ”¹ Transformer (Encoder)

RÃ´le

DÃ©pendances longues sans rÃ©currence

Attention globale

Input

(B, T, D)

Output

(B, T, D)

Keras
MultiHeadAttention(
    num_heads,
    key_dim,
    value_dim=None,
    dropout=0.0
)


blocs usuels :

LayerNormalization

Dense (FFN)

Skip connections

PyTorch
nn.TransformerEncoderLayer(
    d_model,
    nhead,
    dim_feedforward=2048,
    dropout=0.1,
    activation="relu",
    batch_first=True
)

2) Couches de sortie selon la tÃ¢che
ğŸ”¹ Classification multi-classes (1 classe parmi K)

Sortie : Linear/Dense(K)

Activation : softmax (souvent dans la loss)

Limite : pas multi-label

ğŸ”¹ Classification binaire

Sortie : Linear(1)

Activation : sigmoid

Limite : sensible au dÃ©sÃ©quilibre

ğŸ”¹ Classification multi-label

Sortie : Linear(K)

Activation : sigmoid par classe

Limite : labels supposÃ©s indÃ©pendants

ğŸ”¹ RÃ©gression non bornÃ©e

Sortie : Linear(O)

Activation : aucune

Limite : valeurs physiquement impossibles possibles

ğŸ”¹ RÃ©gression bornÃ©e [0,1]

Sortie : Linear(O) + Sigmoid

Limite : saturation proche des bornes

ğŸ”¹ RÃ©gression positive

Sortie : Softplus ou ReLU

Limite : ReLU peut bloquer Ã  0

3) Losses utilisÃ©es dans la littÃ©rature
ğŸ”¹ RÃ©gression

MSE : standard, sensible aux outliers

MAE : robuste, convergence plus lente

Huber / SmoothL1 : compromis idÃ©al

NLL Gaussienne : prÃ©diction Î¼, Ïƒ

ğŸ”¹ Classification

CrossEntropy : multi-classes

Binary Cross Entropy : binaire / multi-label

Focal Loss : classes dÃ©sÃ©quilibrÃ©es

KL Divergence : distributions / distillation

ğŸ”¹ SÃ©quentiel spÃ©cifique

CTC Loss : sÃ©quences non alignÃ©es

Ranking / Contrastive : embeddings

4) Optimizers â€“ fonctionnement et usages
ğŸ”¹ SGD

Descente pure du gradient

Bonne gÃ©nÃ©ralisation

Lent, LR critique

ğŸ”¹ SGD + Momentum

Accumulation de vitesse

TrÃ¨s utilisÃ© en CNN vision

ğŸ”¹ Adam

Moments dâ€™ordre 1 et 2

Rapide, robuste

Standard pour RNN/LSTM

ğŸ”¹ AdamW

Adam + weight decay correct

Standard pour Transformers

TrÃ¨s bon gÃ©nÃ©raliste

ğŸ”¹ RMSProp

Moyenne mobile des gradientsÂ²

Historiquement utilisÃ© pour RNN

ğŸ”¹ Adagrad / Adadelta

Features rares

Peu utilisÃ©s aujourdâ€™hui

ğŸ”¹ LAMB / Adafactor

TrÃ¨s gros modÃ¨les

NLP / Transformers large-scale

5) Associations typiques observÃ©es

CNN (vision) â†’ SGD + momentum / Adam

RNN / LSTM â†’ Adam / RMSProp + gradient clipping

Transformer â†’ AdamW + scheduler + warmup

RÃ©gression â†’ Adam(W) + MSE / Huber

Classification â†’ Adam(W) + CE / BCE
